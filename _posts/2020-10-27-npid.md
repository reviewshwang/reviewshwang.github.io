---
title: "[1] Unsupervised Feature Learning via Non-Parametric Instance Discrimination"
excerpt: "CVPR 2018"

categories:
  - ssl

use_math: true

last_modified_at: 2020-10-27T08:06:00-05:00
---

# Unsupervised Feature Learning via Non-Parametric Instance Discrimination

## Quick Highlights

> ### Contribution & Advantage
> 
> - Suggested benefits non-parametric instance discrimination
> 	
> 	1. As parametric instance classification is not lenient to unseen instance
> 
> 	2. No need to calculate & update gradients of instance-classifying parameters
> 
> - Proved that inducing the network to discriminate between instances can **naturally** lead to underlying clusters. 
> 
> ### Disadvantage
> 


## Introduction

![Fig.1]({{site.url}}/assets/images/npid_1.png){: width="40%" height="40%"}

### Motivation

- Above is tested with model trained with supervision for classification with cross-entropy loss. 

- Image from class *leopard* is rated much higher by class *jaguar* rather than by class *bookcase*.

- Typical discriminative learning method can automatically discover apparent similarity among semantic categories **without being explicitly guided to do so**.

- If we **learn to discriminate between individual instances** even without their labels, we may end up with a representation that captures apparent similarity *among instances*, just like how class-wise supervised learning still retains apparent similarity *among classes*.

## Approach

![Fig.2]({{site.url}}/assets/images/npid_9.png){: width="70%" height="70%"}


### 1. Non-Parametric Softmax Classifier

Why no parametric softmax classifier?

- Parametric approach may not adapt to unseen classes or instances

- Don't need to compare & store gradients of parameters

- Learning general ability to discriminate the unlikes is what we want

- Objective function is designed to yield **similar features from the same image but contrasing features from different images**

![Fig.2]({{site.url}}/assets/images/npid_2.png){: width="20%" height="20%"}

- **v**: encoded features from the memory bank

- \tau : temperature

Learnign objective is to maximize the joint probability 

![Fig.3]({{site.url}}/assets/images/npid_3.png){: width="20%" height="20%"}

or equivantly to minimize the negative log-likelihood

![Fig.4]({{site.url}}/assets/images/npid_4.png){: width="20%" height="20%"}

### 2. Noise-Contrastive Estimation: Reduce computational cost

Why we need Noise-Contrastive Estimation?

- Remind that the task is equivalent to n (number of data) -wise classification task

- Too much computation.

Idea of NCE

- Converting the multi-class classification problem into a set of binary classification problem.

- The binary classification task is to discriminate between *data samples* and *noise samples*. 


![Fig.5]({{site.url}}/assets/images/npid_5.png){: width="20%" height="20%"}

- Calculating Z with above is computationally expensive. So we adopt Monte Carlo approximation.

![Fig.6]({{site.url}}/assets/images/npid_8.png){: width="30%" height="30%"}

- Formalize the noise distribution as a uniform distribution

- Posterior probability of sample *i* with feawture **v** being from the data distribution is straightforwatdly

![Fig.6]({{site.url}}/assets/images/npid_6.png){: width="30%" height="30%"}

- Thus, our objective is to maximize above, or to minimize the negative log-prior distribution of data and noise sample.

![Fig.6]({{site.url}}/assets/images/npid_7.png){: width="20%" height="20%"}

- **Then, the distribution of the same image will be yielded to look similar, and dissimilar to noise distribution**

### Weighted k-Nearest Neighbor Classifier

- After training, inference on a query is done with a conventional method.
$\frac{5}{3}$


