---
title: "[1] Unsupervised Feature Learning via Non-Parametric Instance Discrimination"
excerpt: "CVPR 2018"

categories:
  - ssl

use_math: true

last_modified_at: 2020-10-27T08:06:00-05:00
---

# Unsupervised Feature Learning via Non-Parametric Instance Discrimination

## Quick Summary

### Contribution & Advantage
 
 - Suggested benefits of **non-parametric instance discrimination**
 	
 	1. As parametric instance classification is not lenient to unseen instance
 
 	2. No need to calculate & update gradients of instance-classifying parameters
 
 - Proved that inducing the network to discriminate between instances can **naturally** lead the network to re-direct images into underlying clusters. 
 
 ### Disadvantage
 
	1. No explicit solution to manifying differences between similar-looking instances of different category.


## Motivation

![Fig.1]({{site.url}}/assets/images/npid_1.png){: width="40%" height="40%"}


Above is relative confidence comparisontested with model trained with supervision for classification with cross-entropy loss. Image from class *leopard* is rated much higher by class *jaguar* rather than by class *bookcase*. From this, we may infer that typical discriminative learning method may automatically discover apparent similarity among semantic categories **without being explicitly guided to do so**.

Thus, if we **learn to discriminate between individual instances** as if each instance represents a class, we may end up with a representation that captures apparent similarity *among instances*, just like how class-wise supervised learning still retains apparent similarity *among classes*.


### Non-Parametric Softmax Classifier

![Fig.1]({{site.url}}/assets/images/npid_9.png){: width="40%" height="40%"}


$P(i \given f_{\theta}(x_{i}))=\frac{exp(\mathbf{v}_{i}^{T}f_{\theta}(x_{i}))/\tau}{\sum{j=1}^{n}exp(\mathbf{v}_{i}^{T}f_{\theta}(x_{i}))}$

where $v$ is the features stored in memory bank, and $f_{\theta}(x_{i}))$ is the feature extracted with the current network. Note that every time we compute $f_{\theta}(x_{i}))$, we update them to the memory bank of the corresponding instances. Thus, memory bank is a set of features, where some of them are extracted with the current network update and the rest with the previous network update. 

The intuition behind the above definition of objective function is that **the network will try to update in a way to create constant feature space for the same feature space while keeping the feature space as far away from features of other instances**. This is just the same as the philosophy of supervised classification task where the network is trying to yield feature space that is constant among the same class and different against other classes.

And of course, the objective is to **minimize the negative log-likelihood of the above over the training set**

The author highlights the advantage of non-parametric classification over parametric one. 

One is that parametric approach may not adapt to unseen classes or instances. Since parameters optimized to specific sets of data will adapt to the data-specific features for classification, the parameters may not be appropriate to new instances or classes. However, with non-parametric approach the network learns how to discriminate features extracted between every different instances, which means that it is likely to discriminate well to unseen instances.  

Another benefit is that it obviously do not require to compare or store gradients of parameters. Considering that the number of parameters of famous CNN architectures are mostly concentrated on classification layer, largely because multi-layer perceptron layers do not share weights, keeping track of and updating gradients of classifying parameters are computationally expensive. 

The paper follows with some techniques to reduce computational cost and to stabilize learning process as below

- **Noise-Contrastive Estimation(NCE)**: One of popular techniques (hierarchical softmax, NCE, and negative sampling) to reduce compuational cost

- **Proximal Regularization**: Stabilizes learning process. This is necessary because in our case **every instance is a class**, meaning that each class is visited once per epoch. To solve this issue, proximal regularization L2 regularization term of memory bank updates. 

- For test, **weighted kNN** is conducted to retrieve k-closest instances from query image. So, more precisely speaking Non-parametric softmax training procedure learns how to **map the instances to discriminative feature spaces**, and the actual classification is NOT done by a trained model, but with a conventional methods like kNN or SVM. 

### Some important results

The results well prove the generalizing ability of the suggested model, which was contrasted against the parametric approach. 

![Fig.1]({{site.url}}/assets/images/npid_2.png){: width="40%" height="40%"}

The suggested network is trained with *ImageNet* and tested on *Places* dataset with **NO FINETUNING**. Yet, it showed better performance over other methodologies at that time. The reason behind this outstanding performance can again be emphasized with the advantage of non-parametric softmax classification. 